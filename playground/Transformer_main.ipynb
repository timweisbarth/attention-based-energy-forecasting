{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../utils/\")\n",
    "sys.path.append(\"./../\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "#Modules of src folder\n",
    "from tools import dotdict\n",
    "import run_non_deepl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({})\n",
    "args.model_params = dotdict({})\n",
    "args.train_params = dotdict({})\n",
    "\n",
    "# Data loading\n",
    "args.from_raw = False\n",
    "args.file_name = \"pp_ts_60_sindex_DE_3f.csv\"\n",
    "from_raw_folder = \"raw\" if args.from_raw else \"preproc_non_deepl\"\n",
    "args.path = f\"./../data/{from_raw_folder}/{args.file_name}\"\n",
    "\n",
    "# Preprocessing\n",
    "args.scaler_name = \"std\"\n",
    "args.scaler = {\"std\":StandardScaler()}[args.scaler_name]\n",
    "\n",
    "# Model and its hyperparameters\n",
    "args.model_name = \"vanillaTransformer\"\n",
    "args.model_type = \"deepl\"\n",
    "\n",
    "args.model_params.input_dim = 14\n",
    "args.model_params.n_embed = 24 # Has to be a mulitple of heads\n",
    "args.model_params.heads = 8\n",
    "args.model_params.n_blocks = 2\n",
    "args.model_params.output_dim = None # dependent on forecast_horizon --> specified in for-loop of pipeline\n",
    "\n",
    "\n",
    "args.train_params.batch_size = 128\n",
    "args.train_params.n_epochs = 50\n",
    "args.train_params.learning_rate = 0.5e-3\n",
    "args.train_params.weight_decay = 2e-6\n",
    "\n",
    "\n",
    "# Prediction\n",
    "args.forecast_setting = \"S\"\n",
    "args.cols_to_lag = ['load', 'solar_gen', 'wind_gen'] # TODO: not used by pipeline for lstm\n",
    "args.targets = [['load'], ['solar_gen'], ['wind_gen']]\n",
    "args.window_size = 24 # TODO!!! hard coded in transformer.py (needs change if changed here)\n",
    "args.stride = 1 # Has to be <= min(window_size, forecast_horizon) and stride * integer = window_size,\n",
    "# and stride * integer2 = forecast_horizon\n",
    "args.lead_time = 0 # TODO: Not working yet\n",
    "args.forecast_horizons = [24]\n",
    "\n",
    "# Plotting\n",
    "args.plot_loss = True\n",
    "args.plot = True\n",
    "args.plot_date = '2018-07-01'\n",
    "args.days = 10\n",
    "\n",
    "# Save model\n",
    "args.save_model = False # TODO: Doesnt work for lstm yet\n",
    "args.save_benchmark = False\n",
    "args.date_time = datetime.now().strftime(\"%m-%d-%H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available? False\n"
     ]
    }
   ],
   "source": [
    "print(\"Cuda available?\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Starting to train univariate vanillaTransformer on load for horizons [24] ----------\n",
      "Using cpu device\n",
      "Number of parameters in network = 33793\n",
      "\n",
      "[1/50] Training loss: 0.3667\t Validation loss: 1.0642\n",
      "[2/50] Training loss: 0.2382\t Validation loss: 0.9521\n",
      "[3/50] Training loss: 0.2239\t Validation loss: 1.0116\n",
      "[4/50] Training loss: 0.2101\t Validation loss: 1.5795\n",
      "[5/50] Training loss: 0.1987\t Validation loss: 1.4344\n",
      "[6/50] Training loss: 0.1827\t Validation loss: 0.7776\n",
      "[7/50] Training loss: 0.1729\t Validation loss: 0.8463\n",
      "[8/50] Training loss: 0.1719\t Validation loss: 1.4923\n",
      "[9/50] Training loss: 0.1658\t Validation loss: 1.2786\n",
      "[10/50] Training loss: 0.1638\t Validation loss: 0.8253\n",
      "[12/50] Training loss: 0.1505\t Validation loss: 1.2298\n",
      "[14/50] Training loss: 0.1422\t Validation loss: 1.1007\n",
      "[16/50] Training loss: 0.1379\t Validation loss: 0.6047\n",
      "[18/50] Training loss: 0.1324\t Validation loss: 0.6165\n",
      "[20/50] Training loss: 0.1291\t Validation loss: 0.7493\n",
      "[22/50] Training loss: 0.1254\t Validation loss: 1.1723\n",
      "[24/50] Training loss: 0.1229\t Validation loss: 1.2512\n",
      "[26/50] Training loss: 0.1227\t Validation loss: 0.7348\n",
      "[28/50] Training loss: 0.1202\t Validation loss: 0.6300\n",
      "[30/50] Training loss: 0.1178\t Validation loss: 0.9090\n",
      "[32/50] Training loss: 0.1194\t Validation loss: 0.6810\n",
      "[34/50] Training loss: 0.1193\t Validation loss: 0.8370\n",
      "[36/50] Training loss: 0.1178\t Validation loss: 0.5779\n",
      "[38/50] Training loss: 0.1133\t Validation loss: 0.8870\n",
      "[40/50] Training loss: 0.1136\t Validation loss: 0.7924\n",
      "[42/50] Training loss: 0.1128\t Validation loss: 0.8956\n",
      "[44/50] Training loss: 0.1094\t Validation loss: 0.7647\n",
      "[46/50] Training loss: 0.1107\t Validation loss: 0.5651\n",
      "[48/50] Training loss: 0.1077\t Validation loss: 0.6470\n"
     ]
    }
   ],
   "source": [
    "run_non_deepl.pipeline(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
